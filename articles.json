{"date": "Feb 05 2025", "updates": [{"title": "Noteworthy AI Research Papers of 2024 (Part Two)", "short": "Noteworthy 2024 AI papers: Llama 3's advanced training, inference-time compute scaling, multimodal LLM architectures (NVLM), replicating OpenAI's o1 reasoning, low-precision scaling laws, & Phi-4's synthetic data training.  LLM efficiency and reasoning are key themes.", "long": "### Llama 3: Meta AI's Llama 3 model family, with variants ranging from 8B to 405B parameters, showcases advancements in pre-training and post-training pipelines compared to Llama 2.  Key improvements include a larger vocabulary and grouped-query attention.\n\n### Inference-Time Compute: Scaling LLM test-time compute can be more effective than solely increasing model parameters, particularly for challenging questions. Techniques like generating multiple solutions and adaptive response distribution updates are explored.\n\n### Multimodal LLMs:  NVIDIA's NVLM research compares different multimodal LLM architectures: unified embedding decoder, cross-modality attention, and a hybrid approach.  Each method offers unique advantages depending on the task and image resolution.\n\n### OpenAI's o1 Reasoning:  Microsoft's replication research suggests that OpenAI's o1 model uses 'journey learning' (incorporating trial-and-error) which outperforms 'shortcut learning' on reasoning tasks.  Distillation provides alternative approaches but may hinder fundamental research.\n\n### LLM Precision Scaling Laws: This study extends Chinchilla's scaling laws for optimal LLM size, addressing low-precision training (16-bit and below). Findings suggest that more training data may be detrimental for heavily quantized models.\n\n### Phi-4 and Synthetic Data: Microsoft's Phi-4 model, trained largely on GPT-4o-generated synthetic data, demonstrates performance gains over similarly sized LLMs.  However, reliance on synthetic data alone negatively impacts knowledge-based tasks.", "url": "https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-2"}, {"title": "Noteworthy AI Research Papers of 2024 (Part One)", "short": "Top 6 AI research papers of 2024 (Part 1): Mixtral's MoE, DoRA's improved LoRA, efficient continual pretraining, DPO vs. PPO alignment, LoRA's learning/forgetting trade-off, & the massive FineWeb dataset.  Part 2 coming soon!", "long": "### Noteworthy AI Research Papers of 2024 (Part One)\n\nThis article summarizes six influential AI research papers published between January and June 2024, focusing on Large Language Model (LLM) advancements.  The author, Sebastian Raschka, PhD, emphasizes that the selection is subjective, based on his personal assessment of impact and fascination.\n\n### January: Mixtral's Mixture of Experts (MoE) Approach\n\nMixtral 8x7B, a Sparse Mixture of Experts (SMoE) model, outperformed existing LLMs like Llama 2 70B and GPT-3.5 on various benchmarks, demonstrating the potential of MoE architectures for efficient resource allocation.\n\n### February: Weight-decomposed Low-Rank Adaptation (DoRA)\n\nDoRA improves upon the popular LoRA finetuning method.  By decomposing weight matrices into magnitude and direction, DoRA achieves superior performance and robustness compared to LoRA.\n\n### March: Continual Pretraining Strategies for LLMs\n\nThis paper highlights simple yet effective techniques for continual pretraining: re-warming and re-decaying learning rates and incorporating a small portion of original pretraining data.  These help prevent catastrophic forgetting during continual learning.\n\n### April: DPO vs. PPO for LLM Alignment\n\nA comprehensive study comparing Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO) in LLM alignment. PPO generally outperforms DPO, especially with out-of-distribution data.\n\n### May: LoRA Learns Less, Forgets Less\n\nThis empirical study compares LoRA and full finetuning, revealing a trade-off: full finetuning excels at learning new knowledge but suffers from greater forgetting of prior knowledge, while LoRA retains more of the original knowledge. \n\n### June: The 15 Trillion Token FineWeb Dataset\n\nA massive dataset (15 trillion tokens) released publicly for LLM training, exceeding the size of previously available datasets.  It helps democratize large-scale LLM research.\n\nThe article concludes with a promise of a second part covering July to December 2024, delving into scaling laws, synthetic data, and future projections.", "url": "https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-1"}]}
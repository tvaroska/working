{"date": "Feb 10 2025", "updates": [{"title": "Understanding Reasoning LLMs", "short": "Boost your LLM's reasoning!  Learn 4 key methods: inference-time scaling, pure RL, SFT+RL, & pure SFT+distillation. DeepSeek R1 shows how, while budget-friendly options like Sky-T1 & TinyZero offer exciting alternatives.  #LLM #Reasoning #AI #DeepLearning", "long": "### Four Approaches to Building Reasoning LLMs\n\nThis article explores four main methods for enhancing Large Language Models (LLMs) with reasoning capabilities, focusing on the DeepSeek R1 model as a case study.  The four approaches are:\n\n1.  **Inference-time scaling:** This involves techniques like chain-of-thought prompting to encourage the LLM to generate intermediate reasoning steps during inference, improving accuracy at a higher cost.\n2.  **Pure reinforcement learning (RL):** DeepSeek R1-Zero demonstrated that reasoning can emerge without initial supervised fine-tuning, using accuracy and format rewards in a \"cold start\" training process.\n3.  **Supervised fine-tuning (SFT) and RL:** DeepSeek R1 built upon R1-Zero, adding SFT stages and RL training for better performance, combining human preference and rule-based rewards.\n4.  **Pure SFT and distillation:** DeepSeek R1-Distill showed that fine-tuning smaller LLMs on SFT data from larger models (like DeepSeek R1) creates efficient reasoning models, offering a budget-friendly option. \n\n### Cost-Effective Reasoning Model Development\n\nThe article also addresses developing reasoning models on a budget.  Distillation (approach 4) provides a cost-effective alternative to massive training runs. The Sky-T1 model, trained for $450, illustrates the potential of targeted fine-tuning.  The TinyZero model showcases the surprising reasoning abilities that can emerge from pure RL even in smaller models.\n\n### DeepSeek R1 and OpenAI's o1\n\nDeepSeek R1 and OpenAI's o1 models are compared.  While DeepSeek R1 offers efficient inference, OpenAI's o1's details remain undisclosed, making a precise comparison difficult.  The article speculates o1 may use inference-time scaling and a similar RL/SFT training process. \n\n### Journey Learning\n\nA novel approach called \"journey learning\" is introduced as an alternative to traditional SFT. By including incorrect reasoning steps and corrections in the training data, models might learn to self-correct, leading to more reliable reasoning. ", "url": "https://magazine.sebastianraschka.com/p/understanding-reasoning-llms"}, {"title": "Noteworthy AI Research Papers of 2024 (Part Two)", "short": "Noteworthy AI Research Papers of 2024 (Part 2) highlights Llama 3's upgrades, inference compute scaling's surprising effectiveness, multimodal LLM architecture comparisons, OpenAI's o1 reasoning replication, low-precision training challenges, and Phi-4's use of synthetic data.  #AI #LLM #MachineLearning #DeepLearning #AIResearch", "long": "### Llama 3: A significant upgrade over Llama 2, featuring a larger vocabulary, grouped-query attention, and training on 15 trillion tokens.\n\n### Inference-Time Compute Scaling: A method of enhancing LLM outputs by allocating more compute during inference, potentially surpassing the performance of larger models for easy to medium difficulty questions.\n\n### Multimodal LLMs: A comparison of unified embedding decoder and cross-modality attention architectures, with Nvidia's hybrid approach offering a balance of computational efficiency and accuracy.\n\n### OpenAI's o1 Reasoning: Replication efforts highlight journey learning, a process encompassing the entire trial-and-error process rather than simply the correct solution path, leading to performance improvements.\n\n### LLM Scaling Laws for Precision: An updated version of Chinchilla's scaling laws incorporating low-precision settings, revealing potential negative impacts of excessive training data with low precision quantization.\n\n### Phi-4: A model trained primarily on synthetic data generated by GPT-4o, showing the potential and limitations of synthetic data for improving LLM performance, though more investigation is needed.\n\n### Overall Outlook: The article predicts continued development of multimodal LLMs, focus on computational efficiency in training and inference, and further research into the use of synthetic data in model training.", "url": "https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-2"}, {"title": "Noteworthy AI Research Papers of 2024 (Part One)", "short": "Top 6 AI research papers of 2024 (Part 1) are summarized: Mixtral's MoE, DoRA's improved LoRA, continual pretraining tips, DPO vs. PPO for LLM alignment, LoRA's learning/forgetting trade-off, & the massive FineWeb dataset.  #AI #LLM #Research #DeepLearning", "long": "### Noteworthy AI Research Papers of 2024 (Part 1)\n\nThis article summarizes six influential AI research papers published between January and June 2024, focusing on Large Language Model (LLM) advancements.  The author, Sebastian Raschka, PhD, provides insightful summaries of each paper, highlighting their impact and relevance.\n\n### January: Mixtral's Mixture of Experts (MoE)\n\nMixtral 8x7B, a Mixture-of-Experts (MoE) LLM, outperformed other models like Llama 2 70B and GPT-3.5 on various benchmarks.  The author explains the MoE architecture, where smaller 'expert' networks handle different tasks, enabling efficient resource allocation.\n\n### February: Weight-decomposed LoRA (DoRA)\n\nDoRA improves upon LoRA (Low-Rank Adaptation), a parameter-efficient finetuning method. By decomposing weight matrices, DoRA offers more flexibility and robustness, outperforming LoRA in some cases.  \n\n### March: Continual Pretraining of LLMs\n\nA paper on simple and effective strategies for continual LLM pretraining is discussed.  Key techniques include re-warming/re-decaying learning rates and adding a small portion of original pretraining data to prevent forgetting.\n\n### April: DPO vs. PPO for LLM Alignment\n\nThis research compares Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO) in LLM alignment. PPO generally outperforms DPO, especially with out-of-distribution data; however, DPO's simplicity makes it widely used.\n\n### May: LoRA's Learning and Forgetting\n\nA study reveals LoRA's trade-off: while learning less new information than full finetuning, it retains more of the original knowledge, making it suitable for specific applications.\n\n### June: FineWeb Dataset\n\nThe FineWeb dataset, containing 15 trillion tokens, is a significant resource for training large LLMs.  Its size and principled development using rigorous filtering processes make it stand out among similar datasets.", "url": "https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-1"}, {"title": "LLM Research Papers: The 2024 List", "short": "Due to injury, my planned 2024 AI research summary is delayed.  Here's a handy list of fascinating LLM papers from 2024 to keep you busy over the holidays! #AI #LLM #Research #MachineLearning  [link to article]", "long": "### A Note from the Author\n\nDue to a recent accident, the author, Sebastian Raschka, is unable to complete his planned year-end AI research summary.  He apologizes for any inconvenience and expects to be back to work in the coming weeks.\n\n### Curated LLM Research Papers\n\nIn the meantime, he offers a curated list of LLM-related research papers from 2024.  This list is categorized by month (January-December) and includes links to each paper on arXiv.  The list is designed to offer a broad overview of the current research landscape in the field.\n\n### Additional Resources\n\nReaders are encouraged to explore additional code-heavy materials and bonus resources provided in a GitHub repository linked in the article. The author also plugs his newly released book, \"Build a Large Language Model (From Scratch),\" which provides deeper insight into the workings of LLMs.\n\n### Subscription Information\n\nAhead of AI, the publication, is reader-supported. Readers are encouraged to become a free or paid subscriber to support the author's work and receive new articles when the author is fully recovered.", "url": "https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list"}, {"title": "Understanding Multimodal LLMs", "short": "New article on multimodal LLMs! Learn about 2 key architectures (Unified Embedding Decoder, Cross-Modality Attention), image encoding, and training.  Explore Llama 3.2, Molmo, NVLM, Qwen2-VL, Pixtral, MM1.5, Aria, Baichuan-Omni, and Emu3.  Discover the latest advancements in multimodal AI!", "long": "### Multimodal LLMs Explained\nMultimodal LLMs process various input types (text, images, audio, video).  This article focuses on image-text models.\n\n### Key Architectures\nTwo main approaches exist: Unified Embedding Decoder and Cross-Modality Attention.  The first concatenates image and text embeddings before feeding to a decoder-only LLM (like Llama 3.2 or GPT). The second integrates image and text embeddings within the attention layers of the LLM for improved efficiency.\n\n### Image Encoding\nImage encoders, often pre-trained Vision Transformers (like CLIP or OpenCLIP), convert images into embeddings. Some models like Fuyu learn their embeddings from scratch.\n\n### Model Training\nTraining involves pretraining (often focusing on the image-text adapter or projector) and instruction finetuning.  Some methods freeze image encoders, others don't.  The choice depends on computational efficiency and performance trade-offs, as shown by NVLM's comparison of both architectures.\n\n### Recent Models\nThe article reviews Llama 3.2, Molmo & PixMo, NVLM, Qwen2-VL, Pixtral 12B, MM1.5, Aria, Baichuan-Omni, and Emu3. These models utilize different combinations of approaches and training strategies, highlighting the diversity of current research.\n\n### Conclusion\nMultimodal LLM development involves many design choices.  The best approach depends on specific tasks and computational resources.", "url": "https://magazine.sebastianraschka.com/p/understanding-multimodal-llms"}, {"title": "Building A GPT-Style LLM Classifier From Scratch", "short": "Learn to build a powerful spam classifier by finetuning a pre-trained GPT model! This article shows you how, covering different finetuning methods and insightful experiments.  Get the full details in my new book \"Build a Large Language Model From Scratch\"! #LLM #NLP #TextClassification #SpamDetection #AI", "long": "### Building an LLM Classifier\nThis article details finetuning a pretrained large language model (LLM) for text classification, specifically spam detection.  It emphasizes the simplicity and efficiency of this approach for real-world applications.\n\n### Why Classification?\nText classification addresses numerous practical challenges, including spam filtering, sentiment analysis, and topic labeling.  Finetuning for classification is an accessible introduction to LLM adaptation.\n\n### Finetuning Methods\nTwo main finetuning methods are covered: instruction finetuning (requiring detailed instructions alongside inputs) and classification finetuning (where the model predicts predefined classes directly).\n\n### Model Modification\nThe article focuses on adapting a GPT model.  This involves freezing most layers and replacing the output layer with a smaller one mapping to the desired classes.  The last token's output is used for classification due to the causal attention mask in GPT models.\n\n### Evaluation and Experiments\nExperiments demonstrate the model's effectiveness with high accuracy. Additional experiments explore training different layers, comparing GPT and BERT performance, the impact of the causal mask, model scaling, Low-Rank Adaptation (LoRA), and the effects of padding.  Results highlight the potential for achieving high accuracy with efficient finetuning strategies.\n\n### Book Announcement\nThe article promotes the author's new book, \"Build a Large Language Model From Scratch\", which delves deeper into the process of building and finetuning LLMs.", "url": "https://magazine.sebastianraschka.com/p/building-a-gpt-style-llm-classifier"}, {"title": "Building LLMs from the Ground Up: A 3-hour Coding Workshop", "short": "Learn to build LLMs from scratch in this 3-hour coding workshop!  Video tutorial covers architecture, pretraining, finetuning, & evaluation.  Includes code & resources. #LLM #AI #MachineLearning #DeepLearning #coding", "long": "### Building LLMs from Scratch: A 3-hour Coding Workshop\n\nThis article presents a 3-hour coding workshop on building Large Language Models (LLMs).  The workshop is delivered via a YouTube video, making it easily accessible and convenient to follow along.\n\n### Understanding LLM Development\n\nThe video covers the entire LLM development cycle, from understanding input data and tokenization to implementing the architecture (using GPT-2 and Llama 2 as examples).  Key concepts are explained, including pretraining and instruction finetuning.  The workshop provides hands-on experience in coding an LLM.\n\n### Practical Application and Evaluation\n\nParticipants will learn to load pre-trained weights using libraries like LitGPT, and how to conduct benchmark and conversational performance evaluations.  Practical code examples and explanations are provided throughout the workshop.\n\n### Accessibility and Resources\n\nThe video includes clickable chapter marks for easy navigation.  Supplementary materials, including a book and GitHub repositories with code, are also available for a more in-depth learning experience.\n\n### Supporting the Author\n\nThe author encourages support by purchasing his book \"Build a Large Language Model (From Scratch)\" or subscribing to his Substack magazine. This workshop is a supplementary resource for readers interested in applying the concepts presented in the book.\n\n### Conclusion\n\nThis comprehensive workshop offers a hands-on approach to building and understanding LLMs, suitable for those with some programming experience and a desire to deepen their understanding of AI.", "url": "https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up"}, {"title": "New LLM Pre-training and Post-training Paradigms", "short": "New LLM training is evolving!  Four recent models show diverse approaches: multi-stage pre-training, knowledge distillation, SFT, RLHF/DPO, and rejection sampling. Data quality trumps quantity.  No single best method yet, but exciting innovations abound!", "long": "### New LLM Training Paradigms\nThis article reviews recent advancements in Large Language Model (LLM) training, focusing on four prominent models: Alibaba's Qwen 2, Apple's AFM, Google's Gemma 2, and Meta's Llama 3.  The traditional pre-training-only approach has evolved to include post-training methods like supervised instruction fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) or direct preference optimization (DPO).\n\n### Pre-training Innovations\nMost models employ multi-stage pre-training.  This commonly involves an initial stage, followed by context-lengthening, and sometimes annealing with high-quality data.  The emphasis is shifting towards data quality over quantity.  Knowledge distillation is also gaining popularity, where a smaller model learns from a larger one.  Synthetic data is frequently used to augment datasets.\n\n### Post-training Strategies\nPost-training commonly starts with SFT, refining response accuracy. Then, alignment methods such as RLHF or DPO are used to align LLMs with human preferences.  Rejection sampling, where a model generates multiple responses and a reward model selects the best, is becoming common.  Innovative techniques like Apple's iTeC (Teacher Committee) are being explored.\n\n### Key Takeaways\nThere is no single \"best\" training pipeline.  Multi-stage pre-training and rejection sampling are prevalent.  Data quality is prioritized, and synthetic data is commonly used.  While RLHF and DPO are popular alignment techniques, approaches vary depending on model size and capabilities.\n\n### Supporting the Author\nThe author recommends their books \"Build a Large Language Model (from Scratch)\", \"Machine Learning Q&A\", and \"Machine Learning with PyTorch and Scikit-Learn\" for more in-depth knowledge.  A paid subscription option is also available to support the magazine directly.", "url": "https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training"}, {"title": "Instruction Pretraining LLMs", "short": "New research on instruction finetuning! Magpie generates high-quality LLM datasets cheaply, while a new paper shows instruction data improves pretraining. Google's efficient Gemma 2 and more research highlights are also discussed. #LLMs #InstructionFinetuning #AI #Gemma2 #ML", "long": "### Instruction Finetuning: A Cost-Effective Approach\n\nMagpie presents a novel method for generating high-quality instruction-following datasets for LLMs.  This technique requires only a locally-running LLM (e.g., Llama 3 8B) and a pre-query template; the LLM generates both instructions and responses.  Surprisingly, models fine-tuned on this synthetic data outperform larger, more expensively-trained models like Llama 2 8B Instruct.\n\n### Instruction Finetuning from Scratch\n\nChapter 7 of the author's book, \"Build a Large Language Model From Scratch,\" offers a comprehensive, hands-on guide to instruction finetuning.  It covers the entire pipeline, from data formatting to model evaluation, providing a practical understanding of the process. The code is available on Github.\n\n### Instruction Pretraining\n\nA research paper explores integrating synthetic instruction-response pairs directly into the LLM pretraining stage. By using an instruction synthesizer LLM to process raw text data, they demonstrate enhanced efficiency and improved performance compared to traditional pretraining methods, especially in continual pretraining scenarios (adapting a model to new domains like finance or biomedicine).\n\n### Gemma 2: Efficient LLM Design\n\nGoogle's Gemma 2 LLMs emphasize efficiency over sheer size.  The models utilize sliding window attention, grouped-query attention, and knowledge distillation. The technical report detailing these methods includes valuable ablation studies.\n\n### Other Notable Research\n\nThe article concludes with a list of recent research papers, including work on synthetic data generation, LLM critics for code evaluation, and enhanced retrieval mechanisms.", "url": "https://magazine.sebastianraschka.com/p/instruction-pretraining-llms"}, {"title": "Developing an LLM: Building, Training, Finetuning", "short": "Learn about the entire lifecycle of Large Language Model (LLM) development!  From building and training to finetuning, this 1-hour presentation covers it all.  Plus, learn about evaluating LLMs and useful rules of thumb for development. #LLM #AI #MachineLearning", "long": "### Understanding LLMs\nThis article provides a comprehensive overview of the Large Language Model (LLM) development lifecycle, explained in a concise and accessible manner suitable for mobile consumption.\n\n### Stages of LLM Development\nThe article details the key steps involved in creating an LLM, including data collection, tokenization, model architecture, pretraining, and various finetuning techniques (classification, instruction, preference).\n\n### Dataset Importance\nIt emphasizes the critical role of datasets in LLM training, highlighting the need for high-quality, representative data to ensure model effectiveness.  The selection and preparation of data are crucial for success.\n\n### Model Architecture and Pretraining\nThe article provides a high-level overview of LLM architecture and the significance of pretraining in establishing a strong foundation for the model's capabilities.  It is essential for robust performance.\n\n### Finetuning Techniques\nDifferent finetuning strategies are discussed, each tailored to achieve specific model behaviors and performance improvements.  Choosing the right technique is context-dependent.\n\n### Evaluation of LLMs\nThe process of evaluating LLMs is thoroughly examined, exploring various methods and their limitations.  It's vital for measuring and enhancing model effectiveness.\n\n### Rules of Thumb\nFinally, the article offers practical guidelines and \"rules of thumb\" for pretraining and finetuning LLMs, providing valuable insights for developers and researchers.", "url": "https://magazine.sebastianraschka.com/p/llms-building-training-finetuning"}, {"title": "LLM Research Insights: Instruction Masking and New LoRA Finetuning Experiments", "short": "New LLM research reveals instruction masking may hinder performance, LoRA excels at preserving knowledge, and MoRA offers a compelling alternative for parameter-efficient fine-tuning! #LLM #AI #DeepLearning #MachineLearning #LoRA #MoRA", "long": "### Instruction Tuning With Loss Over Instructions\nThis paper challenges the common practice of masking instructions during instruction tuning of LLMs.  Experiments show that not masking instructions (instruction modeling), while simpler, can lead to better performance, especially with shorter responses and smaller datasets.  The benefit is attributed to reduced overfitting.\n\n### LoRA Learns Less and Forgets Less\nThis study empirically compares LoRA and full fine-tuning for LLMs across programming and math domains.  LoRA learns less, as expected due to its limited parameter updates, but it forgets less of the original model's capabilities after further training on a new domain. This learning-forgetting trade-off depends on the task and how similar it is to the original training data.\n\n### MoRA: High-Rank Updating for Parameter-Efficient Finetuning\nMoRA, a new parameter-efficient finetuning method, uses a high-rank update instead of LoRA's low-rank approach.  MoRA aims to be effective for both instruction tuning and continued pretraining. While offering comparable performance to LoRA in certain tasks, it outperforms LoRA on memory-intensive tasks, suggesting its potential when acquiring new knowledge is necessary.\n\n### Other Interesting Research Papers in May\nThe article lists and briefly summarizes many other LLM research papers from May 2024.  These papers cover topics like new position encoding, model optimization through neural architecture search, memory efficient training, and various aspects of model alignment.  The author points out those he found especially interesting and relevant to his own research.", "url": "https://magazine.sebastianraschka.com/p/llm-research-insights-instruction"}, {"title": "How Good Are the Latest Open LLMs? And Is DPO Better Than PPO?", "short": "April 2024 saw major open LLM releases: Mixtral, Llama 3, Phi-3, and OpenELM!  Llama 3 boasts a huge dataset, while Phi-3 prioritizes data quality.  OpenELM is efficient for mobile.  A new study shows PPO generally outperforms DPO for LLM alignment.  Many more exciting research papers are covered too!", "long": "### April 2024: A Big Month for Open LLMs\n\nFour major open LLMs were released: Mixtral 8x22B, Llama 3, Phi-3, and OpenELM.  Mixtral emphasizes larger models, Llama 3 uses a massive dataset (15 trillion tokens), and Phi-3 prioritizes high-quality data over quantity. OpenELM, from Apple, focuses on efficiency for mobile devices and offers a detailed research paper.\n\n### Comparing Mixtral, Llama 3, and Phi-3\n\nThe author presents a comparison of these LLMs' performance on the MMLU benchmark, illustrating the trade-offs between model size, training data, and performance. Llama 3 shows impressive results due to its massive dataset, while Phi-3 demonstrates that high-quality data can lead to better results than simply increasing the dataset size.\n\n### OpenELM: A Detailed Look\n\nApple's OpenELM stands out for its openness, providing code, datasets, and weights. Key architectural improvements include a layer-wise scaling strategy, gradually widening layers from early to later blocks. LoRA and DoRA methods for parameter-efficient fine-tuning were compared, showing little difference in performance.\n\n### DPO vs. PPO for LLM Alignment\n\nA recent study comprehensively compares DPO and PPO, methods for aligning LLMs. PPO generally outperforms DPO, but DPO is simpler to implement and may be preferable in some situations.  The study also provides best practices for using both methods.\n\n### Other Notable Research\n\nThe author lists many other notable April 2024 research papers, covering various topics, including novel architectures (KANs), improved training methods, and enhanced LLM capabilities for long context learning and code understanding.", "url": "https://magazine.sebastianraschka.com/p/how-good-are-the-latest-open-llms"}, {"title": "Using and Finetuning Pretrained Transformers", "short": "Learn 3 key ways to use & finetune LLMs: feature-based approach, in-context prompting & parameter updates (including soft prompt, prefix, adapter & LoRA).  Improve performance with RLHF! #LLMs #AI #MachineLearning #DeepLearning", "long": "### Using Pretrained Transformers\nThis article explores various methods for utilizing and fine-tuning pretrained large language models (LLMs).  The author highlights three primary approaches:\n\n*   **Feature-based approach:**  This efficient method uses a pretrained LLM as a feature extractor, generating embeddings that are then used to train a simpler downstream model like a linear classifier.  This avoids updating the LLM's parameters.\n*   **In-context learning/prompting:**  This method involves providing examples of the desired task within the input prompt itself, allowing the LLM to infer the correct behavior without any parameter updates.  It's beneficial when labeled data is scarce.\n*   **Finetuning:** This approach involves updating some or all of the LLM's parameters to adapt to a new task.  There are two variations: finetuning only the output layers (more efficient) and finetuning all layers (potentially better performance).\n\n### Parameter-Efficient Fine-tuning\nThe article then delves into parameter-efficient fine-tuning techniques to optimize resource usage while maintaining performance:\n\n*   **Soft prompt tuning:**  Trainable parameter tensors (soft prompts) are prepended to embedded inputs to improve performance.\n*   **Prefix tuning:** Similar to soft prompt tuning, but soft prompts are prepended to each transformer block, improving training stability.\n*   **Adapter methods:**  Add small, trainable adapter layers within the LLM's transformer blocks.\n*   **Low-rank adaptation (LoRA):**  This technique approximates parameter updates using low-rank matrix decompositions, leading to significant parameter reduction.\n\n### Reinforcement Learning with Human Feedback (RLHF)\nFinally, RLHF is discussed as a way to align LLM outputs with human preferences, using human feedback to train a reward model that guides the LLM's learning via reinforcement learning. This method was used for ChatGPT's development.  The author also provides several code examples to implement the different methods mentioned in the article and points to a newly published book.", "url": "https://magazine.sebastianraschka.com/p/using-and-finetuning-pretrained-transformers"}, {"title": "Tips for LLM Pretraining and Evaluating Reward Models", "short": "New research shows continued pretraining is a cost-effective way to update LLMs.  RewardBench, a new benchmark, evaluates reward models in RLHF, revealing the popularity (and potential bias) of DPO methods.  Check out other March 2024 AI papers on LLM efficiency and multimodal reasoning!", "long": "### Continued Pretraining of LLMs\nThis article discusses a research paper exploring cost-effective strategies for continually updating Large Language Models (LLMs) with new data.  Instead of expensive retraining from scratch, the researchers demonstrate that continued pretraining, with careful learning rate scheduling (\"re-warming and re-decaying\") and the inclusion of a small portion of previous data, achieves comparable performance.  This is important for keeping LLMs up-to-date and adaptable to new domains.\n\n### Reward Modeling and RLHF\nThe article also covers reward modeling, a key component of Reinforcement Learning from Human Feedback (RLHF). RLHF is used to align LLMs with human preferences.  The article explains reward modeling and introduces RewardBench, a new benchmark for evaluating reward models, particularly highlighting the trade-off between the computationally intensive RLHF approach with explicit reward models and simpler Direct Preference Optimization (DPO).\n\n### RewardBench Benchmark\nRewardBench offers a new evaluation framework to assess both reward models and DPO models by measuring their accuracy in selecting preferred responses from pairs of outputs. The results show a correlation between reward accuracy and model size for DPO models, but also highlight the dominance of DPO on current leaderboards, likely due to its simplicity and therefore prevalence of trained models.\n\n### Other Notable Research\nFinally, the article lists several other notable AI research papers from March 2024, covering topics such as memory-efficient fine-tuning, efficient attention frameworks, and multimodal reasoning challenges.  These papers suggest ongoing efforts towards improving the efficiency, capabilities, and alignment of LLMs.", "url": "https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms"}, {"title": "A LoRA Successor, Small Finetuned LLMs Vs Generalist LLMs, and Transparent LLM Research", "short": "Exciting month in AI! New open-source LLMs OLMo & Gemma, DoRA (LoRA successor), and insights on small vs large LLMs.  Small models outperform on niche tasks but struggle with longer contexts. Gemma shows impressive results.  Plus, a curated list of other February AI research papers! #AI #LLM #research #DoRA #OLMo #Gemma", "long": "### Research Papers in February 2024\n\nThis month's AI research highlights include two new open-source LLMs, OLMo and Gemma, and a new parameter-efficient finetuning technique, DoRA. OLMo is completely open source, sharing everything from the training code and dataset to log files. Gemma, while not fully open-source, offers openly available weights and achieves state-of-the-art performance.\n\n### Small LLMs vs. Generalist LLMs\n\nA study, \"Tiny Titans\", examines whether small finetuned LLMs (under 2B parameters) outperform larger models.  Results show mixed results, with smaller models excelling in certain niche, in-domain tasks but falling short on larger datasets due to context window limitations. Automated metrics (ROUGE) and human evaluations are discussed as potential sources of bias.\n\n### DoRA: A LoRA Successor\n\nDoRA, a new parameter-efficient finetuning technique, is presented as an alternative to LoRA.  The article includes a link to a deeper dive, covering DoRA's implementation.\n\n### Gemma: Architecture Deep Dive\n\nThe Gemma LLM's architecture is explored, noting similarities to Llama 2 while highlighting unique aspects such as a large vocabulary (256,000 words), extensive training data (6 trillion tokens), and the use of GeGLU activations.  Differences in normalization layers and other architectural choices are discussed and compared to similar models.  The article also provides links to the Gemma technical report and a Lit-GPT implementation.\n\n### Other Interesting Research\n\nA selection of other interesting AI papers published in February 2024 is also included, with some highlighted for special attention.  Topics include 1-bit LLMs, RLHF optimization, and extending LLM context windows. ", "url": "https://magazine.sebastianraschka.com/p/research-papers-in-february-2024"}, {"title": "Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch", "short": "LoRA efficiently fine-tunes LLMs. DoRA improves LoRA by decomposing weights, applying LoRA only to direction, & training magnitude separately.  This article implements both in PyTorch from scratch, demonstrating parameter efficiency and improved performance. Check out the code: [link to GitHub repo]", "long": "### Low-Rank Adaptation (LoRA)\nLoRA is a parameter-efficient technique for fine-tuning large language models (LLMs). It updates only a small subset of model parameters, reducing computational cost and memory usage.\n\n### DoRA: An Improvement on LoRA\nDoRA (Weight-Decomposed Low-Rank Adaptation) is a new method that builds upon LoRA. DoRA decomposes the weight matrix into magnitude and direction components, applying LoRA only to the direction. This improves performance and robustness.\n\n### LoRA Implementation in PyTorch\nThe article provides a step-by-step implementation of a LoRA layer in PyTorch.  It demonstrates how to create the necessary matrices (A and B), apply scaling (alpha), and integrate this into a `Linear` layer to replace existing ones within a larger network.\n\n### Applying LoRA to a Neural Network\nThe implementation is applied to a simple 3-layer Multilayer Perceptron (MLP). The article shows how to replace standard `Linear` layers with custom LoRA-enhanced layers and then freeze the original weights, leaving only the LoRA components trainable.\n\n### DoRA Implementation in PyTorch\nDoRA is implemented by extending the LoRA code.  The key difference is the addition of weight normalization and a learnable magnitude vector (m) for each weight column.  This allows for dynamic scaling and improved learning.\n\n### DoRA's Advantages\nDoRA, even at half the rank of LoRA, often outperforms LoRA and is more robust to hyperparameter adjustments.\n\n### Code Availability\nA GitHub repository containing the complete code for both LoRA and DoRA implementations is provided.", "url": "https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch"}, {"title": "Making the U.S. the home for open-source AI", "short": "Open-source AI is booming, but its future is uncertain. DeepSeek's emergence challenges the US's dominance, highlighting the need for faster iteration and a focus on securing compute resources, not just model weights.  Openness and investment in open research are crucial for a thriving Western AI ecosystem.", "long": "### Open-Source AI's Uncertain Future\nThe article discusses the unexpected rise of DeepSeek, a Chinese open-source AI model, and its implications for the future of AI.  While the open-source AI community champions ideals of accessibility and safety, building a sustainable ecosystem is proving more challenging than creating powerful models.\n\n### High Costs and Geopolitical Tensions\nThe costs associated with training advanced AI models are escalating rapidly, potentially hindering open-source development.  Geopolitical factors also play a significant role, with both US and Chinese leaders emphasizing the importance of their respective nations leading in open-source AI, raising concerns about potential government influence and backdoors in models.\n\n### The Importance of Iteration Speed\nDeepSeek's rapid development and release cycle highlight the importance of iteration speed in the open-source model.  The US needs to prioritize speed and efficiency to compete.\n\n### The Value of Openness\nRestricting access to open-source models through geo-blocking or penalties is a losing strategy.  Openness fosters collaboration and innovation and is essential for a successful ecosystem.  Therefore, the article advocates for maintaining openness to create a thriving AI environment.\n\n### Focus on Compute, not Just Weights\nEfforts to prevent adversaries from using AI should center on controlling access to the significant compute resources required for training and deployment, rather than solely focusing on model weights.\n\n### Investing in Open Research\nThe article concludes by emphasizing the need for greater investment in open-source AI research and public-sector initiatives to develop safe and trustworthy Western alternatives to models like DeepSeek. The focus should be on building a strong ecosystem, rather than only powerful models.", "url": "https://www.interconnects.ai/p/making-the-us-the-home-for-open-source"}, {"title": "Why reasoning models will generalize", "short": "Reasoning models, using chain-of-thought processing, are about to revolutionize AI.  Initial success in coding and math is extending to other domains, producing superior results (even in creative tasks!). Expect higher performance at a price, driving a new era of AI development.", "long": "### Reasoning Models: The Next Leap in AI Generalization\n\nThe author argues that reasoning models, a new generation of AI trained to express extended chain of thought, are poised to significantly surpass current language models in various domains.\n\n### Chain of Thought: The Key to Generalization\n\nChain of thought reasoning, where models break down problems into smaller steps, is seen as the key to better generalization.  This approach is particularly effective in precise tasks like coding and mathematics.\n\n### Beyond Code and Math\n\nWhile initially focusing on code and math, the author believes the benefits of chain of thought will extend to other areas due to its inherent capacity to manage complexity.\n\n###  Superior Performance and Efficiency\n\nThe author predicts reasoning models will be superior in peak performance and more efficient at a broader range of tasks.  They may still lag in cost-effectiveness for complex problems due to higher inference requirements.\n\n### Evidence of Generalization\n\nOpenAI's research on 'Deliberative Alignment' and 'Trading Inference-Time Compute for Adversarial Robustness' provides early evidence that reasoning training generalizes to safety domains, a non-traditional application.\n\n###  Real-World Applications\n\nModels like DeepSeek-R1 have achieved top performance across multiple benchmarks, including creative writing and the challenging 'Humanity's Last Exam'. This suggests reasoning models are capable of exceeding expectations in unexpected areas.\n\n###  The Future of AI\n\nThe author concludes that reasoning models mark a significant shift, potentially driving a price war in the AI industry as models with similar abilities but higher computational requirements compete for market share.  We should expect these models to continuously improve at a much faster rate than anticipated.", "url": "https://www.interconnects.ai/p/why-reasoning-models-will-generalize"}, {"title": "The latest open artifacts (#6): Reasoning models, China's lead in open-source, and a growing multimodal space", "short": "China leads in open-source AI! DeepSeek's R1 model & others are pushing boundaries.  Reasoning models are booming, with new datasets & codebases accelerating innovation.  Check out the latest open artifacts & explore the implications.", "long": "### China's AI Surge\nThe open-source AI landscape has dramatically shifted, with Chinese labs like DeepSeek now leading in model capabilities, surpassing American counterparts.  DeepSeek's V3 and R1 models, along with contributions from Qwen and Minimax, mark a turning point.\n\n### Reasoning Models in Focus\nThe article highlights the rapid advancements in reasoning models.  Several notable models are discussed including Llama-3.2V-11B-cot, deepthought-8b-llama, QwQ-32B, QVQ-72B, and DeepSeek-R1, emphasizing their strengths and potential.\n\n### Key Open-Source Artifacts\nSeveral key models and datasets are featured, such as Bespoke-Stratos-17k (a valuable reasoning dataset), MiniMax-Text-01 (with a huge context window), and ModernBERT-base (a modern take on the classic BERT architecture). The article provides links to these resources.\n\n### Emerging Trends\nThe growing multimodal space and the increasing use of Reinforcement Learning (RL) in fine-tuning models are highlighted as important trends.  The article examines how DPO-like ideas can improve process supervision.  The importance of long-context models, such as Qwen's 1M context window model, is also stressed.\n\n### Geopolitical Implications\nThe article briefly touches upon the geopolitical implications of China's leading role in open-source AI, posing questions about the potential responses from the US government.\n\n### Additional Resources\nThe article includes links to relevant blog posts, interviews, and research papers, providing further insights into the discussed models and trends.  A HuggingFace collection is also provided.", "url": "https://www.interconnects.ai/p/open-artifacts-in-january-6-reasoning"}, {"title": "Interviewing OLMo 2 leads: Open secrets of training language models", "short": "Interview with OLMo team on building open language models.  They discuss training efficiency, stability (including a failed 70B run!), OLMo's role, & data strategies.  Learn about the challenges of open-source model development and their iterative approach to improvement!", "long": "### Interview with OLMo Team Leads\nThis podcast features an interview with the leads of the OLMo project at AI2, discussing their journey in building open-source language models.\n\n### Training Efficiency and Stability\nThe team shares their experience in achieving stability during the pre-training process, including challenges encountered during a failed 70B parameter model run and strategies employed to overcome these challenges.\n\n### OLMo's Role in the AI Landscape\nThe discussion covers the role and evolution of OLMo within the broader AI ecosystem, focusing on its value for research and its potential to provide unique model capabilities.\n\n### Decision Making in Language Model Development\nThe team delves into the intricate decision-making process involved in building language models, from architectural choices and hyperparameter tuning to data acquisition and pre-processing techniques. They emphasize iterative improvement via smaller, more targeted experiments.\n\n### Data and Mid-Training Strategies\nThe importance of data quality and the balance between diverse and targeted data acquisition are explored. This discussion also includes the use of mid-training strategies to optimize training efficiency and improve model capabilities.  \n\n### Release Strategy and Open Science\nThe podcast concludes with insights into release decisions, emphasizing a balance between open access and the responsible release of well-documented models to foster collaboration and build upon.\n", "url": "https://www.interconnects.ai/p/olmo-2-pod"}, {"title": "DeepSeek R1's recipe to replicate o1 and the future of reasoning LMs", "short": "DeepSeek AI's R1, an MIT-licensed reasoning LLM, replicates OpenAI's o1 performance using a 4-stage RL-heavy training process. This open-source approach accelerates RLM research and could spark a price war, marking a significant shift in the field.", "long": "### DeepSeek R1: A Breakthrough in Reasoning Language Models\n\nDeepSeek AI's recent release of DeepSeek R1 marks a significant advancement in reasoning language models (RLMs).  R1, trained through a four-stage process heavily reliant on reinforcement learning (RL), achieves impressive results and is released under the MIT license, encouraging further development by the broader research community.\n\n### The Four Stages of R1 Training\n\nThe training involves a unique four-step approach. It begins with a \n\"cold start\" supervised finetuning phase using synthetic data generated by R1-Zero, an RL-only model. This is followed by extensive large-scale RL training focusing on reasoning problems.  Next, rejection sampling incorporates general queries alongside reasoning problems, broadening the model's capabilities. Finally, a second RL phase blends reasoning and general preference tuning to enhance helpfulness and harmlessness.\n\n### Open-Source Implications\n\nThe MIT license for R1 contrasts sharply with the closed nature of previous leading RLMs like OpenAI's o1, suggesting a shift toward open collaboration in RLM research. This open approach allows for community contributions and accelerates progress. This transparency also impacts pricing, potentially sparking a competitive price war similar to that witnessed in 2023's Mixtral inference price war.\n\n### Key Observations\n\nWhile specific data details remain undisclosed, the success of R1 highlights the importance of strong base models and extensive RL training for robust reasoning capabilities.  The study of R1's training methodology reveals the significance of not only accuracy rewards but also factors such as formatting and language consistency in creating a reliable and user-friendly model.  Future research will explore optimizing the balance between various reward models and refining training procedures.", "url": "https://www.interconnects.ai/p/deepseek-r1-recipe-for-o1"}, {"title": "Let me use my local LMs on Meta Ray-Bans", "short": "Meta Ray-Bans show the potential of AI-powered devices.  But current AI is limited by cloud dependency.  Open-source, local LLMs could unlock true innovation & integration, creating a more natural and enjoyable user experience.  The future is here!", "long": "### Meta Ray-Bans and the Future of Local AI\n\nThe author discusses the potential of AI-powered devices like Meta Ray-Bans, highlighting their user-friendly form factor and the excitement surrounding their capabilities.\n\n### Current Limitations of Meta Ray-Bans\n\nWhile the author finds the device's design excellent, they also point out its limitations in terms of AI functionality, calling the AI \"slightly outdated\" compared to modern chatbots.\n\n### Local vs. Cloud-Based Language Models\n\nThe author explores the differences between local (on-device) and cloud-based language models. They suggest local models are better suited for specific, efficient tasks, while cloud models offer broader capabilities and are crucial for maintaining battery life in current devices.\n\n### Potential of Open-Source Local Models\n\nThe author advocates for open-source local language models and the development of SDKs to allow developers to integrate their models into devices like Meta Ray-Bans. This would create a positive feedback loop for AI innovation and provide users with greater flexibility and control.\n\n### The Larger Technological Trend\n\nThe article concludes by discussing the broader trend of AI integration into everyday devices. The author highlights the potential for AI to seamlessly integrate into our lives, making various tasks easier and more convenient. They express hope that companies will embrace open-source models to accelerate this process.", "url": "https://www.interconnects.ai/p/to-meta-ray-ban-local-ai"}, {"title": "DeepSeek V3 and the actual cost of training frontier AI models", "short": "DeepSeek V3 crushes benchmarks with fewer parameters & lower reported cost than rivals!  But the REAL cost of frontier AI is far higher than $5M.  Efficient engineering, open weights, but not fully open-source - what does this mean for the future of AI?", "long": "### DeepSeek V3: A Powerful, Cost-Effective AI Model\n\nDeepSeek AI's new general-purpose model, DeepSeek V3, boasts impressive performance on challenging benchmarks, exceeding even top models like GPT-4 and Claude 3.5.  It achieves this with only 37 billion active parameters, significantly less than competitors, making it highly attractive for various enterprise applications.\n\n### Efficiency through Innovation\n\nDeepSeek V3's success stems from several key technical innovations: multi-head latent attention (MLA) for minimized memory usage, multi-token prediction for improved performance, efficient mixture-of-experts architecture, and custom multi-GPU communication protocols.  These advancements contribute to its exceptional learning efficiency (performance per FLOP).\n\n### The True Cost of Frontier AI\n\nWhile DeepSeek reports a $5 million cost for the final training run, this figure significantly underrepresents the true cost.  Pre-training experiments, research and development, and a large team of 139 technical authors add up to a far greater expense \u2013 likely in the hundreds of millions of dollars.   Focusing on the final training cost provides a skewed perspective. \n\n### Open-Source Considerations\n\nDeepSeek V3's open weights are a significant development, yet most users are likely to use the model via its website or API rather than directly working with the model weights. While increased transparency aids the development of future models, the full cost benefits of truly open-source AI (open code and data) are not yet realized.\n\n### Future Outlook\n\nThe DeepSeek V3's performance demonstrates that building high-performing AI models is becoming more accessible. Although the current costs remain substantial, continuous advancements are reducing them, potentially leading to DeepSeek V3-level models being trainable for ~$5.5M in a few years. This success also challenges existing power dynamics in AI development. ", "url": "https://www.interconnects.ai/p/deepseek-v3-and-the-actual-cost-of"}, {"title": "(Voiceover) DeepSeek V3 and the actual cost of training frontier AI models", "short": "DeepSeek V3: Training frontier AI is way more expensive than the $5M figure suggests!  Efficiency is key, with DeepSeek V3 outperforming others in benchmarks.  Total costs include hardware, personnel, & data.  Transparency is needed for accurate cost assessments.", "long": "### DeepSeek V3's Efficiency\nDeepSeek V3, a frontier AI model, boasts impressive learning efficiency, outperforming other models in benchmarks.  This efficiency is due to novel training techniques and a focus on optimizing the learning process.\n\n### Compute Cost Transparency\nThe article challenges the commonly cited $5 million figure for DeepSeek V3's training, arguing it's misleading.  The actual cost is significantly more nuanced and depends on various factors beyond a single training run, including infrastructure, personnel, and data acquisition.\n\n###  The True Cost of Frontier AI\nTraining frontier AI models like DeepSeek V3 involves substantial expenses beyond the immediate training costs.  These include infrastructure costs (hardware, energy), personnel costs (researchers, engineers), and data acquisition and preparation costs.  The article emphasizes a more comprehensive view of the total cost of development.\n\n###  Benchmark and ChatBotArena Results\nDeepSeek V3 demonstrates superior performance compared to its counterparts in established benchmarks and the ChatBotArena. This highlights the model's effectiveness and potential for real-world applications.\n\n###  Compute Usage Breakdown\nFigures illustrate a detailed breakdown of DeepSeek V3's compute usage. These figures give a transparent look into the resources required for training and deployment, providing a more complete picture of resource consumption.", "url": "https://www.interconnects.ai/p/voiceover-deepseek-v3-and-the-actual"}, {"title": "The state of post-training in 2025", "short": "Post-training is booming in 2025!  Three key methods are instruction, preference, & reinforcement finetuning.  While costly, AI feedback is reducing data dependency. Mastering post-training is key to advanced reasoning models; open-source replications are coming soon!", "long": "### Post-Training's Prominence in 2025\n\nThe author expresses optimism regarding the advancements in post-training techniques for language models since 2024.  While acknowledging that open post-training methods still lag behind proprietary models like GPT-4, the understanding of the process has significantly improved.\n\n### Three Key Post-Training Methods\n\nThe current landscape of post-training is categorized into three main methods: Instruction finetuning (supervised), Preference finetuning (generalizing Reinforcement Learning from Human Feedback), and Reinforcement finetuning (for task-specific improvement).\n\n### Cost and Data Dependency\nPost-training, although cheaper than pretraining, is becoming more expensive due to increased data costs and computational demands.  However, a shift towards AI-generated feedback is reducing reliance on human data, significantly lowering costs.\n\n### Post-Training's Role in Reasoning Models\n\nThe author emphasizes that mastering post-training techniques is critical for developing advanced reasoning models.  The progress in this area is positive, and open-source replications of cutting-edge models are expected to emerge in the near future, mostly based on post-training techniques.\n\n### Conclusion\n\nOverall, the article paints a positive picture of post-training's future, highlighting its growing importance, cost considerations, and its role in the development of advanced language models.  The author's insights into the cost and data efficiency of post-training suggest a promising path for future open-source developments.", "url": "https://www.interconnects.ai/p/the-state-of-post-training-2025"}, {"title": "Quick recap on the state of reasoning", "short": "LLMs are reasoning, but differently than humans! OpenAI's o1 models & RFT API show large-scale RL improves inference & reasoning. Community replications are emerging, though OpenAI's scale is unmatched.  Reward shaping & sophisticated \"grader\" models are key to advancing LLM reasoning.", "long": "### Reasoning in Large Language Models\n\nThe author explores the evolving understanding of reasoning in LLMs, particularly in light of OpenAI's o1 models and the Reinforcement Fine-Tuning (RFT) API.  The article clarifies the distinction between post-training, reasoning, and inference-time compute, arguing that the latter two are often conflated.\n\n### Defining Reasoning\n\nThe author prefers a broad definition of reasoning: \"the process of drawing conclusions by generating inferences from observations.\" This avoids limiting the discussion to solely human-like reasoning.\n\n### LLMs and Reasoning\n\nThe article contends that LLMs do indeed reason, although in a way different from humans.  They manipulate intermediate tokens during chain-of-thought prompting, essentially performing a form of reasoning via their token output.\n\n### OpenAI's o1 Models and RFT API\n\nOpenAI's o1 models are highlighted as a demonstration of large-scale reinforcement learning used for enhanced inference-time compute and reasoning. The RFT API is presented as a way to apply similar methods with less infrastructure, optimizing for correct answers rather than stylistic features.\n\n### Community Replications and RL\n\nThe author notes several community replications of o1 models, though they are generally narrower in their application and lack the extensive resources of OpenAI. Reinforcement Learning (RL) is highlighted for its ability to enhance specific LLM capabilities without degrading overall performance.\n\n### Reward Shaping and Evaluation\n\nThe role of \"grader\" models in evaluating and refining RL approaches is discussed.  The author points out the increasing complexity of these models as they progress beyond simple binary (correct/incorrect) reward schemes, particularly when dealing with domains like code generation where nuanced quality judgments are needed.", "url": "https://www.interconnects.ai/p/the-state-of-reasoning"}, {"title": "2024 Interconnects year in review", "short": "2024 AI was HUGE! OpenAI's o1 changed everything, ushering in Reasoning Language Models.  My newsletter, Interconnects, thrived, hitting 20K subs & 1.2M views, focusing on RL/post-training, open-source AI, & new model releases.  Open-source faced challenges, but Llama's growing.  Policymakers took notice! 2025 will be even MORE exciting!", "long": "### 2024: A Year in AI Review\n\nThis article summarizes the key trends and events in AI during 2024, as observed by Nathan Lambert through his weekly newsletter, Interconnects.\n\n### OpenAI's o1 Model\n\nThe launch of OpenAI's o1 model is highlighted as a pivotal moment, signifying a shift in the model training and usage paradigm.  This signals the end of scaling as we know it and expands AI beyond chat interfaces.\n\n### Reasoning Language Models (RLMs)\n\nRLMs, spurred by o1, show rapid progress, promising significant changes in 2025. The increased demand for AI extends beyond the limitations of chat-based interfaces.\n\n### Interconnects' Growth\n\nInterconnects itself had a successful 2024, publishing 60 articles, 5 \"Artifacts Log\" posts, 10 interviews, and transitioning to real read-throughs of the AI Voiceovers.  It crossed 20K subscribers and 1.2 million page views.\n\n### Key Focal Points\n\nThe newsletter's focus remained on reinforcement learning (RL) and post-training, open-source AI and policy impacts, and new model releases.\n\n### Open-Source AI's Evolution\n\nOpen-source AI's trajectory was more complex; while initially a key area for companies seeking market share, increased costs in 2024 led to more restrictive licenses. This shift caused open-source to become less relevant to certain players, though Llama continues to gain market share.\n\n### Policy Impact\n\nLambert's work on open-source strategy and policy, while less widely read, directly influenced policymakers, as seen by Interconnects' citation in the House AI Task Force Report.\n\n### Looking Ahead\n\n2025 is predicted to be another significant year for AI, especially in the development of reasoning models and the future of open-source AI. ", "url": "https://www.interconnects.ai/p/2024-interconnects-year-in-review"}, {"title": "(Voiceover) 2024 Interconnects year in review", "short": "2024 AI recap!  Listen to Nathan Lambert's voiceover summary of Interconnects' two-year journey covering AI breakthroughs.  Get the full story & more at [link to article]! #AI #ArtificialIntelligence #YearInReview #Podcast", "long": "### 2024 Year in Review for Interconnects\nThis podcast episode provides a voiceover summary of the year 2024 for the Interconnects publication.  It highlights two years of weekly writing on AI, covering advancements in the field.  The episode is a brief overview, so it is recommended to access the original article for a more detailed understanding.\n\n### Key Themes in AI in 2024\nThe voiceover touches upon major themes in AI during 2024, although specific details aren't included in this brief summary.  To understand these themes, listening to the full podcast episode is necessary.\n\n### Accessing the Original Article\nFor complete information and details, including visuals and more in-depth analysis of AI trends throughout 2024, access the original article linked within the podcast's description.  This detailed review is meant for those who want to go beyond this concise voiceover summary.\n\n### Accessing Related Content\nThe podcast mentions other episodes and articles related to different areas within AI, including Open Language Models, reinforcement learning, and the state of reasoning in the field.  These can be found on the Interconnects website and app.\n\n### Author's Background\nThe podcast is presented by Nathan Lambert, an ML researcher with experience at leading AI companies like Meta, DeepMind, and HuggingFace. His background adds credibility to his analysis of the AI landscape.", "url": "https://www.interconnects.ai/p/voiceover-2024-interconnects-year"}, {"title": "OpenAI's o3: The grand finale of AI in 2024", "short": "OpenAI's o3 model shatters expectations!  Massive improvements in reasoning, coding, and the ARC AGI prize.  This isn't just a new model, it's a whole new level of AI capability, demonstrating rapid progress in AI reasoning and challenging previously held assumptions. #AI #OpenAI #o3 #reasoning #AGI", "long": "### OpenAI's o3 Model: A Breakthrough in Reasoning\n\nOpenAI's newly-previewed o3 model marks a significant leap in AI reasoning capabilities, surpassing expectations and setting a new standard.\n\n### Key Improvements\n\no3 demonstrates substantial improvements across various benchmarks, including the ARC AGI prize (exceeding 85% accuracy), Frontier Math (achieving 25% accuracy), and coding benchmarks like SWE-Bench-Verified (71.7%). This progress comes just three months after the announcement of the o1 model, showcasing rapid advancement in the field.\n\n### Significance of o3\n\nThe o3 model's success suggests that the industry is moving beyond solely relying on internet-scale pretraining.  By significantly enhancing reasoning abilities, o3 is expected to accelerate AI research and potentially transform numerous software engineering roles.\n\n### Cost and Architecture\n\nWhile the exact specifics are yet to be fully revealed, it appears o3 leverages reinforcement learning training and long-context reasoning, possibly with a much larger base model.  It likely uses consensus methods, running multiple parallel responses and selecting the most common, explaining its high inference costs.  There's no definitive evidence that this implies new architecture additions.\n\n### Future Implications\n\nThis progress positions reasoning-based language models, similar to o1, as a pivotal future tool.  The rapid pace suggests widespread adoption and considerable impact on various fields in the coming years, although further development remains necessary.  The capabilities of these models are expected to become widely accessible much faster than predicted.", "url": "https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai"}, {"title": "(Voiceover) OpenAI's o3: The grand finale of AI in 2024", "short": "OpenAI's o3 model is a game-changer in AI, surpassing GPT-4 in reasoning capabilities.  It excels at the ARC benchmark, showcasing its powerful architecture and RL-enhanced performance.  Reasoning language models are the future of AI!", "long": "### OpenAI's o3: A Reasoning Revolution\n\nOpenAI's latest model, o3, is a significant advancement in AI, marking a step change comparable to the release of GPT-4.  This powerful language model excels at reasoning and problem-solving, a crucial step towards more general artificial intelligence.\n\n### Tackling the ARC Challenge\no3 demonstrates impressive performance on the Abstraction and Reasoning Corpus (ARC), a challenging benchmark designed to test AI's abstract reasoning capabilities.  Its success suggests significant progress in bridging the gap between current AI and human-level intelligence.\n\n### o3's Architecture and Training\nThe article delves into o3's architecture, shedding light on its design and the extensive resources required for its training. Notably, it avoids using tree search methods, setting it apart from some other prominent AI models.\n\n### Reinforcement Learning's Comeback\nThe year 2024 witnesses a resurgence of reinforcement learning (RL) techniques in the AI field. o3's development reflects this trend, highlighting RL's importance in enhancing AI's decision-making abilities and improving model performance on complex tasks.\n\n### Figures and Examples\nSeveral figures in the article illustrate o3's performance, showcasing its capabilities on various tasks, including frontier math and coding problems, demonstrating its potential across diverse areas.\n\n### Conclusion: The Future of AI\nThe release of o3 signifies a major leap forward in AI development, showcasing the potential of reasoning language models and the continuing influence of reinforcement learning in shaping the field's future.  This model is a significant milestone on the path to building more capable and versatile AI systems.", "url": "https://www.interconnects.ai/p/voiceover-openais-o3-the-grand-finale"}, {"title": "The AI agent spectrum", "short": "Clearer definitions needed for AI agents beyond simple tool-use models.  A spectrum from basic chatbots to fully agentic systems is outlined, with key questions on online learning, multi-agent interactions, and regulation of open-ended AI agents.", "long": "### AI Agent Spectrum Needs Clarification\nThe article emphasizes the need for clearer definitions and classifications of AI agents. The current broad definition, often linked to reinforcement learning, is considered insufficient.  A more nuanced taxonomy is proposed.\n\n### Agent Classification Spectrum\nA spectrum is presented, ranging from simple agents like search-based chatbots, to more complex ones integrating multiple tools and language models.  The highest level includes agents with open-ended, general access to the user's digital life.\n\n### Beyond Tool Use: Orchestration and Agentic Models\nThe article distinguishes between tool-use language models (basic integration), orchestration language models (multiple integrations), and agentic language models (open-ended tasks).  It points out that current agents are more often orchestration systems rather than true agents.\n\n### Framework for Understanding Agents\nA framework based on four key aspects \u2013 scoping the horizon (planning across time), defining utility (measuring success), pruning information (managing input/output), and interacting with agents (multi-agent systems) \u2013 is suggested as a better approach to classifying and understanding AI agents.  This is a refined version of a previous reinforcement learning framework.\n\n### Questions for the Future\nThe article concludes with questions regarding future trends:  Will language models be trained online? How will multiple agents interact? How will open-ended and closed agents be managed and regulated? What will the user experience be like?", "url": "https://www.interconnects.ai/p/the-ai-agent-spectrum"}, {"title": "(Voiceover) The AI agent spectrum", "short": "Explore the evolution of AI agents from simple reactive systems to complex, model-based agents with multiple feedback loops. This podcast clarifies different agent classes & explores challenges for future development. #AI #Agents #MachineLearning #ReinforcementLearning #Podcast", "long": "### Introduction\nThis podcast episode delves into the spectrum of AI agents, tracing their evolution from reinforcement learning's early days to the present.  It provides a comprehensive overview of different agent classes and their functionalities.\n\n### Agent Cartography\nThe podcast introduces a framework for classifying AI agents based on their capabilities and functionalities. It explains various types of agents such as reactive agents, model-based agents, and agents with various feedback mechanisms. The episode clarifies the distinctions between these classifications, offering a clearer understanding of their applications and limitations.\n\n### Questions for the Near Future\nThe episode explores several key questions and considerations for the future of AI agents. It discusses the challenges, future research directions, and potential breakthroughs in the field. It also covers how different types of agents will continue to adapt and evolve.  The podcast touches upon the potential impacts of these agents on various aspects of life.\n\n### Figures and Diagrams\nThe podcast employs a visual aid, a diagram (Figure 1), to depict how various AI agents leverage different feedback loops and data sources. This diagram serves to illustrate the concepts discussed and enhance comprehension.\n\n### Overall Summary\nThe podcast offers a thorough exploration of the AI agent spectrum, from its foundational principles to the open questions that drive ongoing research and development. It provides valuable insights into the diverse capabilities of different agents and their potential implications.", "url": "https://www.interconnects.ai/p/voiceover-the-ai-agent-spectrum"}, {"title": "OpenAI's Reinforcement Finetuning and RL for the masses", "short": "OpenAI's new Reinforcement Finetuning (RFT) API democratizes reinforcement learning, enabling efficient fine-tuning of LLMs.  This boosts model performance, opens doors for open-source alternatives, and suggests RL's role in AI is evolving beyond a simple add-on.", "long": "### OpenAI's Reinforcement Finetuning API\nOpenAI has released a Reinforcement Finetuning (RFT) API, marking a significant step towards making reinforcement learning (RL) accessible to a wider audience.  This API allows users to fine-tune OpenAI's models, enhancing their performance on specific tasks with minimal parameter updates.\n\n### How RFT Works\nUnlike standard supervised fine-tuning, RFT iterates hundreds or thousands of times over a small dataset, reinforcing desired behaviors.  Users provide training and validation data, along with a 'grader' configuration to define correctness, enabling the model to learn optimal responses.\n\n### Impact of RFT\nThe RFT API suggests several impactful changes: enhanced stability in RL training, enabling broader adoption;  the potential for creating open-source alternatives; a data flywheel for advanced reasoning models like OpenAI's o1, increasing performance by training on diverse user data; and finally, an overall expansion of RL's role in language model training beyond its current 'cherry-on-top' status.\n\n### Technical Details and Hypotheses\nThe API's 'grader' likely functions as reward shaping, allowing for sophisticated answer checking across complex domains such as code.  OpenAI's claims of data efficiency, requiring only 'dozens' of samples, suggests remarkable improvements in RL training stability and efficiency.\n\n### Conclusion\nThe RFT API showcases the growing impact of RL in language model training, potentially shifting its role from a niche technique to an essential component for building effective AI applications.", "url": "https://www.interconnects.ai/p/openais-reinforcement-finetuning"}, {"title": "(Voiceover) OpenAI's Reinforcement Finetuning and RL for the masses", "short": "OpenAI's reinforcement finetuning: democratizing RL! This podcast explores its impact, implementation hypotheses, and implications for the future of AI. #AI #ReinforcementLearning #OpenAI #podcast", "long": "### OpenAI's Reinforcement Finetuning\nThis podcast discusses OpenAI's reinforcement finetuning, a significant advancement in AI.  It's described as the culmination of Yann LeCun's long-held vision. The podcast explores the implications of this technology and its potential to democratize reinforcement learning.\n\n### Impact of Reinforcement Finetuning\nThe release of reinforcement finetuning is considered a major step forward, potentially transforming various fields.  The podcast examines how easily accessible this technology will make advanced RL techniques available to a wider range of developers and researchers.\n\n### Hypotheses on Implementation\nThe podcast delves into how this finetuning is likely implemented, offering informed hypotheses. It analyzes possible techniques used to enhance the efficiency and accessibility of reinforcement learning processes.\n\n### Figures and Visuals\nThe podcast provides visual aids to complement the discussion, such as diagrams and learning curves.  These visuals enhance understanding of the technical concepts explained in the audio.\n\n### Chapters\nThe podcast is divided into helpful chapters for easy navigation and specific topic exploration.  These chapters allow listeners to jump to particular aspects of the subject that are of most interest to them.\n\n### Overall\nThe podcast aims to demystify complex AI concepts by providing clear explanations and contextualization within the broader AI landscape. It offers insights that are both high-level and technically grounded.", "url": "https://www.interconnects.ai/p/voiceover-openais-reinforcement-finetuning"}, {"title": "Interviewing Finbarr Timbers on the \"We are So Back\" Era of Reinforcement Learning", "short": "Reinforcement learning is BACK!  New advancements like OpenAI's O1 & RLHF show huge promise.  Expert Finbarr Timbers & I discuss RL's history, challenges (reward modeling!), & the future of training AI models. Listen now on your favorite podcast platform! #AI #ReinforcementLearning #MachineLearning #OpenAI #DeepMind", "long": "### Reinforcement Learning: A Revisit\nReinforcement learning (RL) is defined as sequential decision-making under uncertainty, involving a trade-off between exploration and exploitation.  The author highlights the \"we're so back\" era of RL, emphasizing its resurgence after a period of relative slowdown.\n\n### Deep RL Breakthroughs\nThe article charts the key advancements in deep RL, starting with Deep Q-Learning (DQN) for Atari in 2013 and continuing to AlphaGo (2016), AlphaZero (2017), and OpenAI Five (2018).  The author notes how these projects showcased RL's potential while also raising challenges, particularly in complex environments requiring significant exploration.\n\n### RLHF and the Slowdown\nThe period following these major projects saw a relative slowdown in large RL initiatives.  This is attributed to the difficulties in exploration and the complexities of imperfect-information games. The introduction of RLHF in 2022 and OpenAI's O1 in 2024 mark a resurgence in RL's prominence.  The author discusses the debate around RLHF, and whether it truly reflects core RL principles.\n\n### Modern Uses of RL\nThe conversation delves into modern applications of RL such as RLHF, OpenAI's O1, and their implications for fine-tuning various machine learning models, including the use of verifiable rewards.   The author explores the challenges of reward modeling and the potential of approaches like Tulu 3, which offer better reward functions that don't merely fit another deep net.\n\n### Building Effective AI Teams\nThe article concludes by discussing the importance of effective team management in AI research and development, emphasizing the need for both highly detailed-oriented individual contributors and managers who can provide the necessary oversight and guidance.", "url": "https://www.interconnects.ai/p/finbarr-timbers"}, {"title": "OpenAI's o1 using \"search\" was a PSYOP", "short": "OpenAI's o1: Is the search hype misleading? New analysis suggests it's powerful RL, not search, driving its reasoning capabilities.  Focus is on outcome-based training and multiple model generations per prompt. #AI #OpenAI #o1 #reinforcementlearning #LLM", "long": "### OpenAI's o1: A Misleading Narrative?\n\nThe author challenges the common perception that OpenAI's o1 model relies on search, suggesting it's primarily a sophisticated reinforcement learning (RL) system.  Initial interpretations, influenced by OpenAI's marketing, emphasized search capabilities.\n\n### The RL-Only Hypothesis\n\nThe author proposes an alternative:  o1's impressive performance stems from large-scale RL training without intermediate rewards or search at test time.  Its apparent search behavior is an artifact of the RL algorithm itself, which implicitly explores strategies to maximize rewards.\n\n### The Importance of Training Data\n\nThe effectiveness of this approach hinges on carefully curated training data emphasizing verifiable answers.  This allows for an outcome-based reward system.  Further refinement occurs through 'continuations,' where the model receives feedback and revises its reasoning steps.\n\n### Controlled Compute, Multiple Generations?\n\nThe article analyzes a key OpenAI graph illustrating model performance versus compute used.  It suggests the graph might be misinterpreted, arguing that it may reflect the distribution of compute across multiple model generations per prompt rather than controlled test-time search. \n\n### A Simpler, Scalable Approach\n\nThe author argues that this RL-centric method, coupled with controlled training data, aligns with Rich Sutton's 'Bitter Lesson', emphasizing the importance of scaling computation through RL rather than explicit search, leading to more robust and coherent AI systems.", "url": "https://www.interconnects.ai/p/openais-o1-using-search-was-a-psyop"}, {"title": "The Evolving Landscape of LLM Evaluation", "short": "LLM evaluation is broken! Benchmarks are quickly saturated, leading to memorization and overfitting.  Subjective user experience ('vibe') is becoming the new metric, but even that's imperfect.  The future needs focused use-case evaluation and human-created, regularly updated benchmarks.", "long": "### The LLM Evaluation Crisis\n\nLarge Language Model (LLM) capabilities have rapidly surpassed existing evaluation benchmarks.  This isn't new, but the problem is worsening.  A small set of benchmarks (MMLU, GMS8k, HumanEval) are now relied upon, yet even these are unreliable.\n\n### Benchmark Saturation and Data Leakage\n\nThe gap between benchmark creation and saturation (when models surpass human performance) has shrunk dramatically.  Models often memorize or overfit to publicly available benchmark datasets. This happens because the models' training data often includes benchmark data found online, leading to artificially inflated scores.\n\n### Mitigating Memorization and Overfitting\n\nSolutions include encrypting datasets, pre-scanning for contamination, and preventing data leakage to closed-source APIs.  However, even with safeguards, some models overfit to benchmarks because of the intense pressure to obtain high scores.\n\n### The Rise of 'Vibe'-Based Evaluation\n\nWith benchmark unreliability, evaluation increasingly relies on subjective user experience, often assessed through platforms like Chatbot Arena.  While crowdsourced, this 'vibe' approach is also imperfect, as user preferences can be unpredictable and influenced by superficial aspects.\n\n### The Future of LLM Evaluation\n\nThe future of LLM evaluation will need to prioritize direct, use-case specific evaluations, rather than relying on public benchmarks.  This demands specialized evaluation expertise, infrastructure, and domain knowledge.  New benchmarks should be human-created and frequently updated to avoid contamination and overfitting.", "url": "https://newsletter.ruder.io/p/the-evolving-landscape-of-llm-evaluation"}, {"title": "Command R+", "short": "Command R+, Cohere's new 104B parameter LLM, tops Chatbot Arena, outperforming GPT-4 versions! Open-weights, excellent RAG, tool use, & multilingual support.  A game-changer for open-source research & enterprise applications! #LLM #OpenSource #AI #NLP #Cohere #CommandRplus", "long": "### Command R+ Model Overview\n\nThis article discusses Command R+, a large language model (LLM) developed by Cohere, achieving top rankings on Chatbot Arena, surpassing even some GPT-4 versions.  It's notable for its open-weight availability, making it accessible for research and potentially commercial use.  The article highlights its superior performance, especially in areas like Retrieval-Augmented Generation (RAG), tool use, and multilingual support. \n\n### Benchmark Performance and Cost-Effectiveness\n\nCommand R+ (104B parameters) excels in various benchmarks. On Chatbot Arena, it outperforms GPT-4 versions at a significantly lower cost. The model's effectiveness in RAG, tool use, and multilingual capabilities, areas often overlooked in Chatbot Arena, is another advantage.  Its speed is also impressive, achieving high token generation rates with proper local deployment. \n\n### Open-Source Accessibility and Research Impact\n\nThe release of Command R+ with publicly available weights represents a major step in bridging the gap between closed-source and open-source models. This accessibility opens significant possibilities for the research community, fostering improvements in model evaluation and development.  The model's non-commercial license also allows exploration for commercial applications by collaborating with Cohere.\n\n### Multilingual Capabilities and Tokenization\n\nTrained on 23 languages with a focus on ten key global business languages, Command R+ handles non-English languages effectively. The advanced, less English-centric tokenizer used by the model improves non-Latin script compression, a factor greatly affecting API costs.  By addressing both the cost and utility issues present in other multilingual models, Command R+ sets a new standard for equitable language support. \n\n### RAG and Tool Use Integration \n\nThe model supports Retrieval-Augmented Generation (RAG) and offers multi-step tool usage capabilities.  The framework helps ensure reliable and verifiable responses, particularly valuable in enterprise environments.  Integration with LangChain for easy tool usage further expands its potential. This combination of functionalities is aimed at providing verifiable and trustworthy responses, which are particularly crucial in enterprise settings.", "url": "https://newsletter.ruder.io/p/command-r"}, {"title": "True Zero-shot MT", "short": "LLMs are nearing human-level translation performance!  This article explores 'true zero-shot MT,' focusing on how LLMs learn to translate low-resource languages using resources like bilingual word lists & grammar books, showcasing the MTOB dataset & the impressive results of Gemini 1.5 Pro.  It also emphasizes the need for long-context datasets & interdisciplinary collaboration.", "long": "### True Zero-shot Machine Translation\n\nRecent advancements in large language models (LLMs) have brought us close to human-level performance in machine translation (MT), particularly on challenging datasets like MTOB.  This article delves into the concept of \n'true zero-shot MT', where models translate into languages unseen during training, relying solely on in-context learning.\n\n### Low-Resource Language Translation\n\nThe article discusses the challenges of translating low-resource languages\u2014those with limited parallel and monolingual data.  Initiatives like WMT and Masakhane aim to bridge this gap by creating resources for these languages.\n\n### Resources for LLM Language Acquisition\n\nThe author explores how LLMs can learn to translate using resources similar to those humans use to learn a second language: bilingual word lists, paired sentences, and grammar books.  PanLex is highlighted as a significant source for bilingual lexicons.\n\n### The MTOB Dataset\n\nMTOB (Machine Translation from One Book) is introduced as a benchmark dataset focused on Kalamang, an endangered language. The dataset includes a bilingual word list, parallel sentences, and a grammar book.  Gemini 1.5 Pro shows improved performance on this task.\n\n### Future Directions\n\nThe article highlights the need for long-context datasets grounded in realistic scenarios.  Multi-modal LLMs will be crucial for languages lacking written traditions. The study underscores the importance of interdisciplinary collaborations between NLP researchers and linguists.", "url": "https://newsletter.ruder.io/p/true-zero-shot-mt"}, {"title": "Building AI Agents from scratch - Part 2: Reflection and Working Memory", "short": "Building AI Agents: Part 2 focuses on reflection and working memory. Learn how to improve AI agent accuracy by enabling self-evaluation and plan revision.  Implement a reflection pattern to fix hallucinations and enhance efficiency, all without orchestration frameworks. Check out the code on GitHub!", "long": "### Reflection in AI Agents\nThe article defines reflection in AI agents as the system's ability to review its outputs and suggest improvements, optionally refining future actions.  It's not a rigidly defined pattern but rather a flexible approach applicable at various stages of an agent's workflow.\n\n### Reflection Methods\nIt explores three reflection methods: a simple one-step feedback loop, a multi-step iterative loop (suitable for code generation), and a plan validation method where the agent checks its execution plan for flaws and corrects hallucinations.\n\n### Memory's Role\nA short-term or working memory is crucial for effective reflection.  The agent needs to recall previous interactions, user queries, and system prompts to contextually evaluate its plans.  The article illustrates a simple working memory implementation using a dataclass to store interactions.\n\n### Practical Implementation\nThe author implements a reflection agent that revises its plan using the working memory and a revised system prompt.  A previous agent example showed how plans could hallucinate incorrect parameters for a currency conversion tool; this improved version uses reflection to correct this error. The working memory and execution flow are designed to correct these issues.\n\n### Pros and Cons of Reflection\nReflection enhances accuracy and flexibility but adds complexity and cost due to extra LLM calls.  The trade-off should be evaluated.  Small models can achieve better results with reflection. The article provides code examples in a GitHub repository.", "url": "https://www.newsletter.swirlai.com/p/building-ai-agents-from-scratch-part-8ca"}, {"title": "Building AI Agents from scratch - Part 1: Tool use", "short": "Build your own AI agent from scratch!  Learn how to integrate LLMs with external tools (Python functions, APIs) for enhanced reasoning.  Master system prompt engineering and build an agent that plans and executes actions. #AI #Agents #LLM #Programming", "long": "### Building AI Agents from Scratch\nThis article is the first in a series that will guide you through building AI agents from scratch without using any frameworks. The goal is to provide a clear understanding of the underlying mechanisms involved.\n\n### Tool Use in AI Agents\nAI agents utilize LLMs for reasoning, but LLMs alone cannot execute code.  The article explains how agents use tools\u2014functions, databases, APIs, even other agents\u2014to solve user requests.  Prompt engineering is crucial, involving crafting effective system prompts with tool definitions and expected outputs.\n\n### Implementing the Agent\nThe core of the article is implementing a Python-based AI agent capable of currency conversion. It uses a custom decorator to turn Python functions into tools. The design involves extracting essential information (name, description, parameters) from functions and packaging it into JSON to feed the LLM.\n\n### System Prompt\nA detailed JSON-formatted system prompt instructs the LLM on the agent's capabilities and expected responses.  It enforces the use of tools only when necessary and dictates the structure of the LLM's output in JSON format (includes steps, tool calls, and results).\n\n### Agent Class and Execution\nThe agent class orchestrates the whole process: adding tools, planning (through LLM), and executing tool calls. The article shows how to execute two example user queries: one requiring tool usage, the other a direct response from the LLM.\n\n### Conclusion\nThis part successfully implements a rudimentary AI agent that leverages external tools to fulfill user intentions. The methodology highlights the importance of system prompt design and the intricacies of integrating LLMs with external functionality.", "url": "https://www.newsletter.swirlai.com/p/building-ai-agents-from-scratch-part"}, {"title": "AI Clouds and their role in the AI era", "short": "AI Clouds are revolutionizing AI by offering cost-effective, high-throughput GPU resources.  Learn how to deploy an open-source LLM chatbot on Nebius AI Cloud and compare the TCO of AI Clouds vs. proprietary APIs. #AICloud #LLM #Nebius #Kubernetes #OpenSourceAI", "long": "### AI Clouds: The New Cloud Computing Era\nAI Clouds are cloud computing platforms optimized for AI workloads, particularly those requiring significant GPU resources.  The demand for GPUs has skyrocketed with the rise of AI, leading to the development of these specialized platforms.\n\n### AI Cloud vs. Proprietary LLM APIs\nThere's ongoing debate about whether to use proprietary LLM APIs (e.g., OpenAI, Anthropic) or AI Clouds for inference.  Proprietary APIs offer ease of use and scalable access but can be expensive, with costs tied to token usage. AI Clouds, on the other hand, offer more control, potentially lower costs at scale, and granular control over throughput and latency.  The choice depends on the application's needs and traffic volume.\n\n### Total Cost of Ownership (TCO)\nTCO is crucial when deciding between proprietary APIs and AI Clouds.  Proprietary APIs have fluctuating costs based on usage; AI Clouds have fixed costs (with autoscaling) that can be more economical at high throughput. AI Clouds allow tuning to lower latency.\n\n### Hands-on Project: Mistral-7B Chatbot\nThe article includes a practical project deploying a Mistral-7B powered chatbot on Nebius AI Cloud.  It covers Kubernetes cluster setup, deployment of an open-source LLM (Mistral-7B-Instruct) using vLLM, chatbot creation with Streamlit, and external exposure via LoadBalancer service.\n\n### Choosing the Right Approach\nThe best choice depends on several factors. For pre-training and finetuning, AI Clouds are essential. For prototyping, proprietary APIs are suitable. However, for cost-effective large-scale applications, AI Clouds are advantageous. This hands-on project helps readers gain practical experience.", "url": "https://www.newsletter.swirlai.com/p/ai-clouds-and-their-role-in-the-ai"}, {"title": "What is AI Engineering?", "short": "AI Engineering is evolving rapidly with LLMs!  AI Engineers bridge ML & Software Engineering, integrating LLMs into robust, non-deterministic systems.  Key skills include research, prompt engineering, and adapted MLOps.  High demand and salaries are predicted, particularly for full-stack engineers.", "long": "### What is AI Engineering?\n\nThe article explores the evolving role of AI Engineering, particularly in the context of Large Language Models (LLMs).  It emphasizes that AI systems haven't fundamentally changed; LLMs offer new capabilities (planning, content generation, code generation) within existing systems.\n\n### AI Engineering vs. Other Roles\n\nAI Engineering differs from traditional Machine Learning (ML) and Software Engineering.  While ML engineers focus on model accuracy and MLOps, and software engineers excel in deterministic systems, AI engineers bridge the gap. They integrate LLMs and other components, build robust, non-deterministic systems, and implement necessary evaluations and observability.\n\n### Essential Skills for AI Engineers\n\nSuccess in AI engineering demands a diverse skillset. This includes research (understanding LLMs and related papers), prompt engineering (crafting and evaluating prompts), software development (building reliable systems), infrastructure knowledge (deploying and managing systems, understanding data stores), data engineering (processing and integrating data), and adapted MLOps (evaluation and observability of agentic systems).\n\n### The Future of AI Engineering\n\nThe future looks bright for AI engineers.  The increasing adoption of agentic systems across industries will drive high demand for these skills.  Full-stack AI engineers will be particularly well-positioned for success, both within established companies and in building new startups. The author predicts 2025 for widespread adoption of agents and 2026-2027 for multi-agent and autonomous systems.", "url": "https://www.newsletter.swirlai.com/p/what-is-ai-engineering"}, {"title": "Memory in Agent Systems", "short": "Agent systems need memory! This article explores short-term & long-term (episodic, semantic, procedural) memory in GenAI agents, highlighting challenges & implementation strategies for building autonomous, cost-effective AI.", "long": "### Memory in Agent Systems\nThis article discusses the implementation of memory in Generative AI (GenAI) systems, focusing on agent systems.  Agent systems, unlike basic LLMs, require memory to solve complex tasks autonomously.\n\n### Types of Agentic Memory\nThe article draws parallels between human memory and agentic memory, categorizing it into short-term (working) memory and long-term memory (further divided into episodic, semantic, and procedural).\n\n### Short-Term Memory\nShort-term memory provides immediate context to the agent, crucial for decision-making.  Challenges in managing it include LLM context window limitations and cost optimization for multiple agent interactions.\n\n### Long-Term Memory\nLong-term memory in agents comprises episodic, semantic, and procedural components. Episodic memory stores interaction history and actions, similar to RAG systems but internally generated. Semantic memory holds external knowledge accessible to the agent and contextual information for grounding actions.  Procedural memory encapsulates the agent's predefined structure, tools, and guardrails. \n\n### Conclusion\nEfficient memory management is vital for building effective agent systems.  The article highlights that the field is still evolving, urging readers to research memory implementation in various frameworks to avoid unexpected issues.", "url": "https://www.newsletter.swirlai.com/p/memory-in-agent-systems"}, {"title": "Observability in LLMOps pipeline - Different Levels of Scale", "short": "Observability in LLMOps is crucial for GenAI systems.  As complexity increases (from RAG to multi-agent networks), so do the scaling needs of observability infrastructure. Tracing and evaluation are key, addressing challenges of variable data, non-determinism, and distributed systems.", "long": "### Introduction\nThis article discusses observability in LLMOps pipelines, focusing on the scaling needs of observability infrastructure as the complexity of GenAI systems increases.\n\n### GenAI Value Chain\nThe GenAI value chain is divided into foundation model training (pre-training and post-training) and GenAI systems engineering (fine-tuned models, RAG systems, agents, and multi-agent networks).\n\n### Observability Challenges\nObservability in advanced GenAI systems centers on tracing and evaluation.  Tracing involves capturing metadata about the end-to-end application flow and its constituent spans (atomic actions).  Challenges include handling variable-length data, non-deterministic behavior, and the need for fine-grained evaluation.\n\n### RAG Systems\nA naive RAG system's trace includes spans for query submission, embedding, ANN lookup, prompt construction, and LLM processing.  Tracing is crucial for debugging, cost estimation, and system evaluation.\n\n### Agents\nLLM-based agents add non-determinism; actions are determined at runtime, potentially leading to unpredictable trace lengths and structures.  Observability must account for diverse span types and complex capabilities (knowledge, memory, tools).\n\n### Multi-Agent Systems\nMulti-agent systems introduce distributed tracing challenges.  Connecting traces from multiple agents, handling varied communication patterns, and preventing infinite loops become crucial considerations.\n\n### Conclusion\nThe evolution of GenAI systems demands advanced observability tools capable of big data analytics, comprehensive tracing, and fine-grained evaluations to handle non-deterministic behavior and complex workflows.", "url": "https://www.newsletter.swirlai.com/p/observability-in-llmops-pipeline"}]}
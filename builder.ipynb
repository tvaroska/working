{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4fbb46-53a2-45d6-984f-56eedfebaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "from article.sources import sources\n",
    "from article.utils import flatten_openapi, generate_extract\n",
    "\n",
    "import mistune\n",
    "from IPython.display import Markdown\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2fa2568-901b-4d8a-869e-e0a597bd9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGuide(BaseModel):\n",
    "    audience: str\n",
    "    style: str\n",
    "    recommendations: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e318d81-f7eb-4bd3-8df3-8035245f79d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Section(BaseModel):\n",
    "    name: str = Field(title='Name/title of the section')\n",
    "    points: List[str] = Field(title='List of no more than 3 main points for this section')\n",
    "    summary: str = Field(title=\"Summary of the section in 20-50 words\")\n",
    "    experts: List[str] = Field(title=\"List of 3-5 experts roles/descrptions who can help with article by providing feedback\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'## {self.name}\\n{self.summary}\\n\\n'+','.join(self.points)\n",
    "\n",
    "class Outline(BaseModel):\n",
    "    title: str = Field(title=\"Title of the article\")\n",
    "    summary: str = Field(title=\"Summary of the article in 20-50 words\")\n",
    "    sections: List[Section]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'# {self.title}\\n{self.summary}\\n\\n' + '\\n'.join([str(x) for x in self.sections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07a51f4-a3b4-4bb5-b61a-b3cb51c0b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = sources()\n",
    "style_articles = [x[1] for x in articles if x[0].startswith('data/oreilly')]\n",
    "articles = [x[1] for x in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722d494-f5b8-4663-ac06-6a8093a4fcb5",
   "metadata": {},
   "source": [
    "### Get style guid\n",
    "\n",
    "Replicate article style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d629abc2-6221-4434-821c-9b4d9cc3bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    ('system', 'You are analyzing articles. You sources are' + '\\n'.join(style_articles)),\n",
    "    ('user', 'Give concise descrition of audience and style of those articles. Write it as recommendation for another authors to follow when writting simmilar articles. Create sections: Audience, Style, Recommendations. Use Heading 2 for section')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d08dc8-232c-4036-b16d-bc21ef18a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = ChatVertexAI(model='gemini-1.5-pro-002', temperature=2, top_k=40, top_p=1)\n",
    "parse_model = ChatVertexAI(model='gemini-1.5-flash-002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31e4e4e5-e626-4849-a4b5-34547a737f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e176995dab4c11b0ec7f2cbbe12c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "styles = []\n",
    "for i in tqdm(range(10)):\n",
    "    style = generate_extract(prompt, gen_model, StyleGuide, parse_model)\n",
    "    styles.append(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1388aa9c-3309-46b7-abf6-1ae50f73949a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**I. Tactical Advice (Effective LLM Development):**\n",
       "\n",
       "* **Prompt Engineering:**\n",
       "    * Start with fundamentals: few-shot, chain-of-thought, and resource integration.\n",
       "    * Concise and focused prompts are better than complex, multipurpose ones. Decouple prompts where relevant to evaluate test impacts across isolated single cases across user needs, test individual specialized cases\n",
       "    * Structure I/O formats consider implementation for maintainability interoperability LLM stacks\n",
       "        *  **Instructor:** Best for language model API access/integration cases. Use SDK to simplify API interfacing LLM generate public corpus and HF repos; prioritize open source dataset use instructor accordingly - based on how intend model' production \n",
       "\n",
       "        *   **Outlines:** Effective for locally hosted HuggingFace models deployments to structure testing feedback loop Llms prompts responses evaluation easier locally maintain; create your hugging-face public data. Create also models that simplify structuring for further building blocks based LL stacks if your company wants make it to the marketplace or enable downstream integrations by developer building using this tech\n",
       "\n",
       "* **Information Retrieval (RAG):**  Combine, leverage keyword and also multimodal search vector emebddings eval benchmark which gives greatest increase based resources versus  simpler keuword where there is negligable improvements by adding RAG to outputs generated relevance and downstream processes\n",
       "\n",
       "keep minimal ensure largest model sizes fits still generates concise RAG index outputs by focusing text actually added/processed avoid excess verbose data, use multiple scoring RAG including keuword MRR, NGCG as metircs or own production data to avoid overly rely metrics if output dont fit purpose to impro RAG per reduce sizes models used \n",
       "\n",
       " prioritize RAG data optimizeing sizes content reduce context/windows size model requests which improves also token compute response latency overall\n",
       "\n",
       "\n",
       " choose/test simpler or bm simple me only implement RAG/ebmedd systems truly adds measurable perf improve on results (instead use always complex and test performance if metrics) prioritizing by metric, test then evalute accordingly on small chuncks to make debug simpler in multi-prompt queries to generate results when chain LL\n",
       "\n",
       "\n",
       "\n",
       "  ensure balanced retrieval cost\n",
       "\n",
       "\n",
       " fine tunes based business compliance not blindly each type application since tuning specialize niche task better result for long rung because model baseline will contin adapt shift. Ensure gains adding to pipelines. Prioritze custom domain models data where pub ones  generate good-outputs but your data too spec so needed by you LL enhance  model apps\n",
       "\n",
       "\n",
       "\n",
       "\n",
       " **II. Strategic Decisions**: pretraining general only if re depend clear  gains case compliance\n",
       "\n",
       "\n",
       "\n",
       " prioritize use current specialized available products outsource compute costs LL hosted already and implement fine only needed due private setup in productions unless huge gains value justifies build maintena from scratch full ML. dev LL pretrn platform for your own L data\n",
       "\n",
       "\n",
       "enhance existing systems workflows. Avoid automation LL if dont gen needed output quality use (use instead focused, specialized uses that give benefit over existing process). Centaury type. tools human focus\n",
       "\n",
       " evaluate your needs business then dev prior prompt test; user eval; iterate refine\n",
       "\n",
       "\n",
       "use trend models as costs to evaluate viability timelines projects L generate products when are feasible (game $ now LL powering to generate in games in under$ with pacman ar example cost compute) based trend analysis\n",
       "\n",
       "\n",
       " focus release value before generic demos test on various prompts and output evaluation on wide range queries prioritize production and results produced then presentation for\n",
       "\n",
       "\n",
       "\n",
       " LLms are soft build invest to manage L stack as it also soft: needs regular upkeep maintanence; testing  e eeach layers refinement refine after production release check outputs feedback users and dont assume prompt one shot \"do then deploy\" wins cycle test feedback loops when implement  b avoid surprises\n",
       "\n",
       "\n",
       "\n",
       "* other key takeaways/aspects:* avoid generalize; specialize specific outputs types based use cases prompts prioritize ship deep user valuable interactions enhance work rather replaceing prioritize smaller specialized to release finished functional soft with user interate build feed  add complexity tuning needed for task to increase perf needed if is needed evaluate benchmarks accordingly on each implemented systems with given prod use constraints first and then release prioritizing actual user experience benefits than blindly following just latest developments/method trends"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(styles[9][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48d3653f-d36d-46bf-9299-8ccc88ecb2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False, False, False, False, False]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[styles[i][1] == styles[i-1] for i in range(1, 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefe63e-f4e5-458e-92be-2320b8816ea3",
   "metadata": {},
   "source": [
    "### Create outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3a0fb9b-4858-4dd7-96fd-86de8a7fa101",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = \"\"\"\n",
    "genai in legacy environment\n",
    "\n",
    "what is different to startup?\n",
    "- existing processes, do we want to automate (potenialy inefective process) or optimize\n",
    "- existing applications - are APIs ready for GenAI?\n",
    "- old documentation - if I have final1.doc and final2.doc which one has valid info?\n",
    "\n",
    "even with the the challenges there is tremendous benefits of GenAI in legacy env\n",
    "\n",
    "What aproaches works:\n",
    "- start small, focused. Having 100s ideas are great but focus is important\n",
    "- start with business - what outcomes do we want and how to measure them\n",
    "- translate it to scenarios - input -> output. with this start evals\n",
    "\n",
    "Evals\n",
    "Human -> llm as a judge\n",
    "\n",
    "Prompt engineering\n",
    "Split to small steps. Easier to manage, easier to evaluate and easier to \"explain\" = less black box feeling\n",
    "\n",
    "Logging\n",
    "log everything, log authomaticaly. Don't trust applications to log on their side\n",
    "\n",
    "Security\n",
    "by default assume not-safe\n",
    "\"\"\"\n",
    "prompt = [\n",
    "    ('system', 'You are preparing to write new article. Follow the instruction in the schema. You can chance title and/or provided summary. You sources are' + '\\n'.join(articles)),\n",
    "    ('user', f'Prepare outline and plan for new article. Initial thoughts {initial}')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7ceedfb-a2a6-476a-84af-290b9696852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_model = ChatVertexAI(model='gemini-1.5-pro-002').with_structured_output(schema=flatten_openapi(Outline.schema()), method='json_mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "464c11da-31e2-44ec-a578-3a965fbe2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = o_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f1f5ee-e496-4841-9282-4dfce9b07a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "oresponse = Outline.parse_obj(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f62aa612-557f-4573-8eea-503664847154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Generative AI in Legacy Environments: Challenges, Strategies, and Best Practices\n",
       "This article explores the specific challenges and considerations of implementing generative AI in legacy environments, offering effective strategies, approaches, and best practices for successful integration.\n",
       "\n",
       "## Introduction\n",
       "This section sets the stage by highlighting the unique challenges and considerations of integrating GenAI in legacy environments.\n",
       "\n",
       "What makes generative AI implementation different in a legacy environment compared to a startup?,Existing processes: automate or optimize?,API readiness for GenAI integration in existing applications,Challenges with outdated documentation\n",
       "## Benefits of GenAI in Legacy Environments\n",
       "This section emphasizes the potential benefits and return on investment of GenAI implementation despite inherent challenges.\n",
       "\n",
       "Discuss the potential advantages and value propositions of adopting GenAI within established organizations\n",
       "## Effective Approaches for GenAI Integration\n",
       "This section outlines practical strategies for successful GenAI adoption in legacy systems.\n",
       "\n",
       "Start small and focused: prioritize key areas for initial implementation,Business-driven approach: align GenAI initiatives with desired outcomes and measurable metrics,Scenario-based planning: define specific input-output scenarios for targeted development and evaluation\n",
       "## Evaluation Strategies for GenAI\n",
       "This section explores different evaluation methods to ensure quality and reliability of GenAI outputs.\n",
       "\n",
       "Human evaluation,LLM as a judge for automated assessment\n",
       "## Prompt Engineering Techniques\n",
       "This section focuses on prompt engineering best practices to enhance GenAI performance and control.\n",
       "\n",
       "Splitting complex tasks into smaller, manageable steps,Improving prompt clarity for reduced ambiguity and better explainability\n",
       "## Logging and Monitoring for GenAI\n",
       "This section discusses logging strategies to gain insights into GenAI behavior and performance.\n",
       "\n",
       "Importance of comprehensive logging for debugging and analysis,Automated logging mechanisms for consistent data capture,Building logging systems independent of application-specific logging\n",
       "## Security Considerations for GenAI\n",
       "This section highlights the importance of robust security measures in GenAI implementations.\n",
       "\n",
       "Default to a not-safe assumption: prioritize security measures from the outset,Address potential vulnerabilities and security risks associated with GenAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(str(oresponse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97ac1232-584b-47f4-bc99-6f970e98b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = prompt\n",
    "prompt2.append(('ai', str(oresponse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "267e2604-b4f8-4bc5-aebe-5f7ffe99d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2.append(('user', 'Change first section to more focus on difference betwen building app in new environment to doing the same thing in legacy (existing processes, API not ready for such apps, documents not avialable etc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a1c7b6f-92c1-475a-818f-a07a200bb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = o_model.invoke(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89317237-61f2-4f9b-8c1b-ea09c0d2131b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sections': [{'experts': ['Software engineers',\n",
       "    'Data scientists',\n",
       "    'IT architects'],\n",
       "   'name': 'Challenges of Building GenAI Apps in Legacy Environments',\n",
       "   'points': ['Existing processes and APIs may not be compatible with GenAI applications.',\n",
       "    'Limited documentation can hinder integration efforts.',\n",
       "    'Data silos and security concerns pose additional challenges.',\n",
       "    'Talent gaps in AI/ML expertise can slow down development.'],\n",
       "   'summary': 'Building GenAI applications in legacy environments differs significantly from new environments. Existing processes, APIs, and documentation may not be ready for GenAI integration, requiring extra effort.'},\n",
       "  {'experts': ['Cloud architects', 'Data engineers', 'Security specialists'],\n",
       "   'name': 'Strategies for Successful GenAI Integration',\n",
       "   'points': ['Modernization and cloud migration are crucial for compatibility.',\n",
       "    'Data integration and management break down data silos.',\n",
       "    'Security and compliance must be addressed throughout the process.',\n",
       "    'Scalable and reliable systems ensure long-term success.',\n",
       "    \"Innovation and experimentation are key to maximizing GenAI's potential.\"],\n",
       "   'summary': 'Overcoming legacy environment challenges requires a multi-faceted approach. Modernization, data management, security, scalability, and innovation are key strategies for successful GenAI integration.'},\n",
       "  {'experts': ['Business analysts', 'Project managers', 'AI product managers'],\n",
       "   'name': 'Real-World Case Studies',\n",
       "   'points': ['[Company A] transformed customer service by integrating GenAI into its legacy CRM.',\n",
       "    '[Company B] automated legacy processes, saving time and resources.',\n",
       "    '[Company C] leveraged AI-powered insights to drive innovation and improve decision-making.'],\n",
       "   'summary': 'Real-world examples illustrate how companies are effectively integrating GenAI into legacy environments, showcasing the potential benefits and challenges.'}],\n",
       " 'summary': 'This document explores the challenges and strategies for building GenAI applications in legacy environments, providing practical advice and real-world examples. It emphasizes the strategic importance of modernization, data management, and security in successful GenAI integrations.',\n",
       " 'title': 'My Experience with Building GenAI Applications in Legacy Environments'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b938a6b-bd8f-4f19-86de-9ccd71d0612f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
